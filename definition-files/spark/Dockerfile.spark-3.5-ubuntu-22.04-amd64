FROM docker.io/mkandes/ubuntu:22.04-amd64

# Add OCI annotations
LABEL org.opencontainers.image.ref.name='docker.io/mkandes/spark:3.5-ubuntu-22.04-amd64'

LABEL org.opencontainers.image.base.name='docker.io/mkandes/ubuntu-22.04-amd64'
LABEL org.opencontainers.image.base.digest='sha256:6028e17f16fe1fa905d351656690676eb2bb457ba84e4100adcc56ed92e61ec2'

LABEL org.opencontainers.image.title='spark'
LABEL org.opencontainers.image.description='Apache Spark is a unified engine for large-scale data analytics'
LABEL org.opencontainers.image.url='https://spark.apache.org'
LABEL org.opencontainers.image.documentation='https://spark.apache.org/docs/latest'
LABEL org.opencontainers.image.source='https://github.com/apache/spark.git'
LABEL org.opencontainers.image.vendor='Apache Software Foundation (https://www.apache.org)'
LABEL org.opencontainers.image.licenses ='Apache-2.0 (https://www.apache.org/licenses)'
LABEL org.opencontainers.image.version='3.5.1'
LABEL org.opencontainers.image.revision='fd86f85e181fc2dc0f50a096855acf83a6cc5d9c'

LABEL org.opencontainers.image.authors='Marty Kandes (mkandes@sdsc.edu)'

# Set the default shell for the image
SHELL ["/bin/bash", "-c"]

# Set operating system mirror URL
ENV MIRRORURL='http://us.archive.ubuntu.com/ubuntu'

# Set operating system version
ENV OSVERSION='jammy'

# Set system locale
ENV LC_ALL=C

# Set debian frontend interface
ENV DEBIAN_FRONTEND='noninteractive'

# Set Spark version and install path
ENV SPARK_MAJ='3'
ENV SPARK_MIN='5'
ENV SPARK_REV='1'
ENV SPARK_VER="${SPARK_MAJ}.${SPARK_MIN}.${SPARK_REV}"
ENV SPARK_HADDOP_MAJ='3'
ENV SPARK_HADOOP_MIN=''
ENV SPARK_HADOOP_REV=''
ENV SPARK_HADOOP_VER="${SPARK_MAJ}"
ENV SPARK_SHA512='3d8e3f082c602027d540771e9eba9987f8ea955e978afc29e1349eb6e3f9fe32543e3d3de52dff048ebbd789730454c96447c86ff5b60a98d72620a0f082b355'
ENV SPARK_ROOT_DIR='/opt/spark'
ENV SPARK_INSTALL_DIR="${SPARK_ROOT_DIR}/${SPARK_VER}"

# Set PATH to Spark commands and scripts
ENV PATH="${SPARK_INSTALL_DIR}/bin:${PATH}"
ENV PATH="${SPARK_INSTALL_DIR}/sbin:${PATH}"

# Set other environment variables utilized by Spark
ENV JAVA_HOME='/usr'
ENV SPARK_HOME="${SPARK_INSTALL_DIR}"
ENV PYTHONPATH="${SPARK_HOME}/python/lib/pyspark.zip:${PYTHONPATH}"
ENV PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:${PYTHONPATH}"

# Upgrade all software packages to their latest versions
RUN apt-get -y update \
 && apt-get -y upgrade

# Install Spark dependencies
RUN apt-get -y install python3 \
 && apt-get -y install python3-dev \
 && apt-get -y install python-is-python3 \
 && apt-get -y install openjdk-17-jdk

# Change working directory
WORKDIR /tmp

# Install pip
RUN wget https://bootstrap.pypa.io/get-pip.py \
 && python get-pip.py \
 && rm get-pip.py

# Install additional (optional) Spark dependencies
RUN python -m pip install numpy \
 && python -m pip install pandas \
 && python -m pip install pyarrow

# Install other common python packages
RUN python -m pip install scipy \
 && python -m pip install scikit-learn \
 && python -m pip install matplotlib \
 && python -m pip install jupyter \
 && python -m pip install jupyterlab

# Download and install a pre-built Spark distribution
RUN wget "https://archive.apache.org/dist/spark/spark-${SPARK_VER}/spark-${SPARK_VER}-bin-hadoop${SPARK_HADOOP_VER}.tgz" \
 && wget "https://archive.apache.org/dist/spark/spark-${SPARK_VER}/spark-${SPARK_VER}-bin-hadoop${SPARK_HADOOP_VER}.tgz.sha512" \
 && sha512sum -c "spark-${SPARK_VER}-bin-hadoop${SPARK_HADOOP_VER}.tgz.sha512" \
 && tar -xf "spark-${SPARK_VER}-bin-hadoop${SPARK_HADOOP_VER}.tgz" \
 && cd "spark-${SPARK_VER}-bin-hadoop${SPARK_HADOOP_VER}" \
 && mkdir -p "${SPARK_INSTALL_DIR}" \
 && mv * "${SPARK_INSTALL_DIR}"

# Change working directory
WORKDIR /tmp

# Remove pre-built Spark installation tarball
RUN rmdir "spark-${SPARK_VER}-bin-hadoop${SPARK_HADOOP_VER}" \
 && rm "spark-${SPARK_VER}-bin-hadoop${SPARK_HADOOP_VER}.tgz"

# Store information about the container image build system
RUN mkdir -p /opt/.nxis \
 && cd /opt/.nxis \
 && df -ahPT > df.default \
 && fdisk --list > fdisk.default \
 && lsblk --output-all > lsblk.default \
 && lsblk --output-all --json > lsblk.json \
 && lscpu --output-all > lscpu.default \
 && lscpu --output-all --json > lscpu.json \
 && lshw > lshw.default \
 && lshw -json > lshw.json \
 && lshw -short > lshw.short \
 && lspci > lspci.default \
 && lspci -vvv > lspci.verbose \
 && free -h > free.default \
 && cat /proc/meminfo > meminfo.default 

# Cleanup
RUN apt-get -y autoremove --purge \
 && apt-get -y clean \
 && rm -rf /var/lib/apt/lists/*

# Update database for mlocate
RUN updatedb
